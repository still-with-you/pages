<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="BlendX: Complex Multi-Intent Detection with Blended Patterns - Yoon et al.">
  <meta name="description" content="BlendX is a multi-intent detection dataset suite with diverse blended patterns and new complexity metrics for task-oriented dialogue.">
  <meta name="keywords" content="multi-intent detection, task-oriented dialogue, dataset, BlendX, LREC-COLING 2024, conversational AI">
  <meta name="author" content="Yejin Yoon, Jungyeon Lee, Kangsan Kim, Chanhee Park, Taeuk Kim">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="HYU-NLP Lab">
  <meta property="og:title" content="BlendX: Complex Multi-Intent Detection with Blended Patterns">
  <meta property="og:description" content="BlendX is a multi-intent detection dataset suite with diverse blended patterns and new complexity metrics for task-oriented dialogue.">
  <meta property="og:url" content="https://still-with-you.github.io/pages/blendx/">
  <meta property="og:image" content="https://still-with-you.github.io/pages/blendx/static/images/blendx_overview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="BlendX overview figure">
  <meta property="article:published_time" content="2024-05-01T00:00:00.000Z">
  <meta property="article:author" content="Yejin Yoon">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="multi-intent detection">
  <meta property="article:tag" content="dataset">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="BlendX: Complex Multi-Intent Detection with Blended Patterns">
  <meta name="twitter:description" content="BlendX is a multi-intent detection dataset suite with diverse blended patterns and new complexity metrics for task-oriented dialogue.">
  <meta name="twitter:image" content="https://still-with-you.github.io/pages/blendx/static/images/blendx_overview.png">
  <meta name="twitter:image:alt" content="BlendX overview figure">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="BlendX: Complex Multi-Intent Detection with Blended Patterns">
  <meta name="citation_author" content="Yoon, Yejin">
  <meta name="citation_author" content="Lee, Jungyeon">
  <meta name="citation_author" content="Kim, Kangsan">
  <meta name="citation_author" content="Park, Chanhee">
  <meta name="citation_author" content="Kim, Taeuk">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="LREC-COLING 2024">
  <meta name="citation_pdf_url" content="https://aclanthology.org/2024.lrec-main.218.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>BlendX: Complex Multi-Intent Detection with Blended Patterns</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/svg+xml" href="static/images/favicon.svg">
  <link rel="apple-touch-icon" href="static/images/apple-touch-icon.png">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="../static/css/bulma.min.css">
  <link rel="stylesheet" href="../static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="../static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="../static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="../static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="../static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="../static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="../static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="../static/js/fontawesome.all.min.js"></script>
  <script defer src="../static/js/bulma-carousel.min.js"></script>
  <script defer src="../static/js/bulma-slider.min.js"></script>
  <script defer src="../static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "BlendX: Complex Multi-Intent Detection with Blended Patterns",
    "description": "BlendX is a multi-intent detection dataset suite with diverse blended patterns and new complexity metrics for task-oriented dialogue.",
    "author": [
      {
        "@type": "Person",
        "name": "Yejin Yoon",
        "affiliation": {
          "@type": "Organization",
          "name": "HYU-NLP Lab"
        }
      },
      {
        "@type": "Person",
        "name": "Jungyeon Lee",
        "affiliation": {
          "@type": "Organization",
          "name": "HYU-NLP Lab"
        }
      },
      {
        "@type": "Person",
        "name": "Kangsan Kim",
        "affiliation": {
          "@type": "Organization",
          "name": "HYU-NLP Lab"
        }
      },
      {
        "@type": "Person",
        "name": "Chanhee Park",
        "affiliation": {
          "@type": "Organization",
          "name": "HYU-NLP Lab"
        }
      },
      {
        "@type": "Person",
        "name": "Taeuk Kim",
        "affiliation": {
          "@type": "Organization",
          "name": "HYU-NLP Lab"
        }
      }
    ],
    "datePublished": "2024-05-01",
    "publisher": {
      "@type": "Organization",
      "name": "LREC-COLING 2024"
    },
    "url": "https://still-with-you.github.io/pages/blendx/",
    "image": "https://still-with-you.github.io/pages/blendx/static/images/blendx_overview.png",
    "keywords": ["multi-intent detection", "task-oriented dialogue", "dataset", "BlendX", "LREC-COLING"],
    "abstract": "Task-oriented dialogue (TOD) systems are commonly designed with the presumption that each utterance represents a single intent. However, this assumption may not accurately reflect real-world situations, where users frequently express multiple intents within a single utterance. While there is an emerging interest in multi-intent detection (MID), existing in-domain datasets such as MixATIS and MixSNIPS have limitations in their formulation. To address these issues, we present BlendX, a suite of refined datasets featuring more diverse patterns than their predecessors, elevating both its complexity and diversity. For dataset construction, we utilize both rule-based heuristics as well as a generative tool—OpenAI’s ChatGPT—which is augmented with a similarity-driven strategy for utterance selection. To ensure the quality of the proposed datasets, we also introduce three novel metrics that assess the statistical properties of an utterance related to word count, conjunction use, and pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art MID models struggle with the challenges posed by the new datasets, highlighting the need to reexamine the current state of the MID field.",
    "citation": "Yoon et al. 2024. BlendX: Complex Multi-Intent Detection with Blended Patterns.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://still-with-you.github.io/pages/blendx/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "multi-intent detection"
      },
      {
        "@type": "Thing",
        "name": "task-oriented dialogue"
      },
      {
        "@type": "Thing",
        "name": "datasets"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "HYU-NLP Lab",
    "url": "https://github.com/HYU-NLP",
    "logo": "https://still-with-you.github.io/pages/blendx/static/images/favicon.ico",
    "sameAs": [
      "https://github.com/HYU-NLP"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">
              <span class="title-line title-line-primary">BlendX:</span>
              <span class="title-line">Complex Multi-Intent Detection with Blended Patterns</span>
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://still-with-you.github.io/" target="_blank">Yejin Yoon</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="mailto:jungyune@hanyang.ac.kr">Jungyeon Lee</a><sup>1</sup>,</span>
              <span class="author-block">Kangsan Kim<sup>1</sup>,</span>
              <span class="author-block">Chanhee Park<sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://galsang.github.io/" target="_blank">Taeuk Kim</a><sup>1*</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup><a href="https://sites.google.com/view/hyu-nlp/" target="_blank">Hanyang University</a></span>
              <span class="author-block"><sup>2</sup><a href="https://www.hyundai.com/worldwide/en" target="_blank">Hyundai Motor Company</a></span>
              <span class="author-block conference-line"><br>LREC-COLING 2024</span>
              <span class="author-block corresponding-line"><br><sup>*</sup>Corresponding author</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <div class="link-row">
                  <span class="link-block">
                    <a href="https://aclanthology.org/2024.lrec-main.218/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>ACL Anthology</span>
                    </a>
                  </span>


                  <span class="link-block">
                    <a href="https://github.com/HYU-NLP/BlendX" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/HYU-NLP/BlendX" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-database"></i>
                      </span>
                      <span>Dataset</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2403.18277" target="_blank"
                    class="external-link button is-normal is-rounded is-muted">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="static/pdfs/blendx-slides.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-muted">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Slides</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="static/pdfs/blendx-poster.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-muted">
                      <span class="icon">
                        <i class="fas fa-image"></i>
                      </span>
                      <span>Poster</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
  </div>
</section>


<!-- Teaser image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure class="image">
        <img src="static/images/blendx_overview.png" alt="BlendX overview figure" loading="lazy">
      </figure>
      <h2 class="subtitle has-text-centered">
        Overview of the BlendX construction framework across ATIS, Banking77, CLINC150, and SNIPS.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Task-oriented dialogue (TOD) systems are commonly designed with the presumption that each utterance represents a single intent. However, this assumption may not accurately reflect real-world situations, where users frequently express multiple intents within a single utterance. While there is an emerging interest in multi-intent detection (MID), existing in-domain datasets such as MixATIS and MixSNIPS have limitations in their formulation. To address these issues, we present BlendX, a suite of refined datasets featuring more diverse patterns than their predecessors, elevating both its complexity and diversity. For dataset construction, we utilize both rule-based heuristics as well as a generative tool—OpenAI’s ChatGPT—which is augmented with a similarity-driven strategy for utterance selection. To ensure the quality of the proposed datasets, we also introduce three novel metrics that assess the statistical properties of an utterance related to word count, conjunction use, and pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art MID models struggle with the challenges posed by the new datasets, highlighting the need to reexamine the current state of the MID field.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Narrative -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths content overview-content">
        <h2 class="title is-3 overview-title">Overview</h2>

        <h3 class="title is-4">Motivation</h3>
        <div class="narrative-block pref-align">
          <div class="narrative-text">
            <br>
            <p>
              Task-oriented dialogue (TOD) systems typically assume that each user utterance corresponds to a single intent, but real-world interactions frequently violate this assumption. Multi-intent detection (MID) aims to address this gap, yet benchmarks such as MixATIS and MixSNIPS often exhibit overly simplistic merging patterns (e.g., limited conjunction templates), which can provide shallow cues for models. BlendX is motivated by the need for a more rigorous and diverse MID testbed that goes beyond naive concatenation and better captures complex blended patterns found in natural conversations.
            </p>
          </div>
          <div class="narrative-figures">
            <figure class="figure-card section-figure figure-shift-right figure-xlarge">
              <img src="static/images/blendx-motivation.png" alt="Motivation for multi-intent detection in BlendX" loading="lazy">
              <figcaption>Figure 1. From MixX to BlendX: beyond naive concatenation.</figcaption>
            </figure>
          </div>
        </div>

        <h3 class="title is-4">Dataset Construction</h3>
        <p>
          BlendX extends four single-intent datasets:
          <a href="https://aclanthology.org/H90-1021/" target="_blank"><strong>ATIS</strong></a>,
          <a href="https://arxiv.org/abs/1805.10190v3" target="_blank"><strong>SNIPS</strong></a>,
          <a href="https://aclanthology.org/2020.nlp4convai-1.5/" target="_blank"><strong>Banking77</strong></a>, and
          <a href="https://aclanthology.org/D19-1131/" target="_blank"><strong>CLINC150</strong></a>,
          producing <strong>BlendATIS</strong>, <strong>BlendSNIPS</strong>, <strong>BlendBanking77</strong>, and <strong>BlendCLINC150</strong>.
        </p>
        <figure class="figure-card section-figure full-width figure-shift">
          <img src="static/images/blendx-concat.png" alt="BlendX concatenation patterns" loading="lazy">
          <figcaption>Figure 3. Concatenation taxonomy in BlendX.</figcaption>
        </figure>
        <p>
          The BlendX concatenation space is defined by two axes—complexity (explicit vs. implicit) and methodology (naive/manual vs. generative)—covering diverse blended patterns such as omissions, coreferences, and gerund phrases.
        </p>
        <p>
          These axes motivate three complementary construction approaches: a naive approach with explicit concatenation using AND-variant connectors, a manual approach with rule-based heuristics and broader conjunction patterns (including omission and gerund phrasing), and a generative approach that uses ChatGPT to produce more natural, implicit merges while preserving original intents. To model realistic complexity, BlendX covers both explicit concatenation and implicit patterns such as ambiguities, omissions, and coreferences.
        </p>
        <figure class="figure-card section-figure full-width figure-shift">
          <img src="static/images/blendx-approaches.png" alt="BlendX construction approaches" loading="lazy">
        </figure>
        <p>
          To improve LLM-based generation quality, BlendX applies a similarity-based selection strategy using SBERT cosine similarity, selecting utterance pairs above a threshold before merging. This reduces intent distortion errors compared to random selection.
        </p>
        <p>
          For quality control, BlendX defines three custom metrics for word count change (W), conjunction change (C), and pronoun change (P). These metrics filter low-quality generations, followed by expert review to remove failures such as intent removal, intent change, or unsuccessful merges.
        </p>

        <figure class="figure-card section-figure full-width figure-shift">
          <img src="static/images/blendx-metrics-sample.png" alt="BlendX metric examples" loading="lazy">
          <figcaption>Table 3. Examples of blended patterns and their metric signatures.</figcaption>
        </figure>
        <p>
          Representative explicit and implicit concatenation cases in BlendX (e.g., ambiguity, gerund phrases, omissions, coreferences), with corresponding values of the three custom metrics (W, C, P).
        </p>

        <h3 class="title is-4">Evaluation</h3>
        <p>
          BlendX is designed to stress-test MID systems under distribution shifts. Models trained and evaluated on MixX can perform well, but performance drops sharply on BlendX, indicating that MixX is not sufficiently challenging. Even when training on BlendX, results do not fully recover to MixX-level performance, suggesting that BlendX contains intrinsically harder patterns and requires stronger MID modeling.
        </p>
        <figure class="figure-card section-figure full-width figure-shift">
          <img src="static/images/blendx-eval.png" alt="BlendX evaluation results" loading="lazy">
          <figcaption>Table 6. Benchmark performance on MixX vs. BlendX.</figcaption>
        </figure>
        <p>
          Accuracy of competitive MID models under different train/test splits, highlighting substantial performance drops when evaluating on BlendX and revealing brittleness under distribution shift.
        </p>

        <h3 class="title is-4">Visualization</h3>
        <p>
          We visualize semantic structure with SBERT embeddings and t-SNE projections to compare MixX and BlendX distributions.
        </p>
        <figure class="figure-card section-figure figure-shift figure-tight">
          <img src="static/images/blendx-vis.png" alt="BlendX visualization" loading="lazy">
          <figcaption>Figure 6. Embedding-space visualization of MixX vs. BlendX.</figcaption>
        </figure>
        <p>
          The projection shows that BlendX remains semantically grounded while exhibiting broader and more diverse distributions than MixX.
        </p>

        <h3 class="title is-4">Resources</h3>
        <p>
          BlendX and MixX (v1.0 English) along with KoBlendX and KoMixX (v2.0 Korean) are available via the repository and Hugging Face dataset hub.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End narrative -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@inproceedings{yoon-etal-2024-blendx-complex,
  title = "{B}lend{X}: Complex Multi-Intent Detection with Blended Patterns",
  author = "Yoon, Yejin and Lee, Jungyeon and Kim, Kangsan and Park, Chanhee and Kim, Taeuk",
  editor = "Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen",
  booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
  month = may,
  year = "2024",
  address = "Torino, Italia",
  publisher = "ELRA and ICCL",
  url = "https://aclanthology.org/2024.lrec-main.218",
  pages = "2428--2439"
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            Built with the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> (adapted from <a href="https://nerfies.github.io" target="_blank">Nerfies</a>).
            Licensed under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
