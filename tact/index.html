<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Beyond Task-Oriented and Chitchat Dialogues: Proactive and Transition-Aware Conversational Agents - Yoon et al.">
  <meta name="description" content="TACT is a transition-aware dialogue dataset unifying task-oriented and chitchat modes, with new Switch/Recovery metrics and strong gains with DPO.">
  <meta name="keywords" content="dialogue systems, task-oriented dialogue, chitchat, transition-aware, dataset, EMNLP, TACT, DPO, conversational agents">
  <meta name="author" content="Yejin Yoon, Yuri Son, Namyoung So, Minseo Kim, Minsoo Cho, Chanhee Park, Seungshin Lee, Taeuk Kim">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="HYU-NLP Lab">
  <meta property="og:title" content="Beyond Task-Oriented and Chitchat Dialogues: Proactive and Transition-Aware Conversational Agents">
  <meta property="og:description" content="TACT is a transition-aware dialogue dataset unifying task-oriented and chitchat modes, with new Switch/Recovery metrics and strong gains with DPO.">
  <meta property="og:url" content="https://still-with-you.github.io/pages/tact/">
  <meta property="og:image" content="https://still-with-you.github.io/pages/tact/static/images/tact_overview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="TACT overview figure">
  <meta property="article:published_time" content="2025-11-01T00:00:00.000Z">
  <meta property="article:author" content="Yejin Yoon">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="dialogue systems">
  <meta property="article:tag" content="transition-aware dialogue">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Beyond Task-Oriented and Chitchat Dialogues: Proactive and Transition-Aware Conversational Agents">
  <meta name="twitter:description" content="TACT is a transition-aware dialogue dataset unifying task-oriented and chitchat modes, with new Switch/Recovery metrics and strong gains with DPO.">
  <meta name="twitter:image" content="https://still-with-you.github.io/pages/tact/static/images/tact_overview.png">
  <meta name="twitter:image:alt" content="TACT overview figure">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Beyond Task-Oriented and Chitchat Dialogues: Proactive and Transition-Aware Conversational Agents">
  <meta name="citation_author" content="Yoon, Yejin">
  <meta name="citation_author" content="Son, Yuri">
  <meta name="citation_author" content="So, Namyoung">
  <meta name="citation_author" content="Kim, Minseo">
  <meta name="citation_author" content="Cho, Minsoo">
  <meta name="citation_author" content="Park, Chanhee">
  <meta name="citation_author" content="Lee, Seungshin">
  <meta name="citation_author" content="Kim, Taeuk">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="EMNLP 2025">
  <meta name="citation_pdf_url" content="https://aclanthology.org/2025.emnlp-main.672.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>Beyond Task-Oriented and Chitchat Dialogues: Proactive and Transition-Aware Conversational Agents | TACT</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/png" href="static/images/favicon.png">
  <link rel="apple-touch-icon" href="static/images/apple-touch-icon.png">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="../static/css/bulma.min.css">
  <link rel="stylesheet" href="../static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="../static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="../static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="../static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="../static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="../static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="../static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="../static/js/fontawesome.all.min.js"></script>
  <script defer src="../static/js/bulma-carousel.min.js"></script>
  <script defer src="../static/js/bulma-slider.min.js"></script>
  <script defer src="../static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Beyond Task-Oriented and Chitchat Dialogues: Proactive and Transition-Aware Conversational Agents",
    "description": "TACT is a transition-aware dialogue dataset unifying task-oriented and chitchat modes, with new Switch/Recovery metrics and strong gains with DPO.",
    "author": [
      {
        "@type": "Person",
        "name": "Yejin Yoon",
        "affiliation": {
          "@type": "Organization",
          "name": "HYU-NLP Lab"
        }
      },
      {
        "@type": "Person",
        "name": "Yuri Son",
        "affiliation": {
          "@type": "Organization",
          "name": "HYU-NLP Lab"
        }
      },
      {
        "@type": "Person",
        "name": "Namyoung So",
        "affiliation": {
          "@type": "Organization",
          "name": "HYU-NLP Lab"
        }
      },
      {
        "@type": "Person",
        "name": "Minseo Kim",
        "affiliation": {
          "@type": "Organization",
          "name": "HYU-NLP Lab"
        }
      },
      {
        "@type": "Person",
        "name": "Minsoo Cho",
        "affiliation": {
          "@type": "Organization",
          "name": "HYU-NLP Lab"
        }
      },
      {
        "@type": "Person",
        "name": "Chanhee Park",
        "affiliation": {
          "@type": "Organization",
          "name": "HYU-NLP Lab"
        }
      },
      {
        "@type": "Person",
        "name": "Seungshin Lee",
        "affiliation": {
          "@type": "Organization",
          "name": "HYU-NLP Lab"
        }
      },
      {
        "@type": "Person",
        "name": "Taeuk Kim",
        "affiliation": {
          "@type": "Organization",
          "name": "HYU-NLP Lab"
        }
      }
    ],
    "datePublished": "2025-11-01",
    "publisher": {
      "@type": "Organization",
      "name": "EMNLP 2025"
    },
    "url": "https://still-with-you.github.io/tact-project-page/",
    "image": "https://still-with-you.github.io/pages/tact/static/images/tact_overview.png",
    "keywords": ["dialogue systems", "task-oriented dialogue", "chitchat", "transition-aware", "dataset", "EMNLP", "TACT", "DPO"],
    "abstract": "Conversational agents have traditionally been developed for either task-oriented dialogue (TOD) or open-ended chitchat, with limited progress in unifying the two. Yet, real-world conversations naturally involve fluid transitions between these modes. To address this gap, we introduce TACT (TOD-And-Chitchat Transition), a dataset designed for transition-aware dialogue modeling that incorporates structurally diverse and integrated mode flows. TACT supports both user- and agent-driven mode switches, enabling robust modeling of complex conversational dynamics. To evaluate an agent's ability to initiate and recover from mode transitions, we propose two new metrics: Switch and Recovery. Models trained on TACT outperform baselines in both intent detection and mode transition handling. Moreover, applying Direct Preference Optimization (DPO) to TACT-trained models yields additional gains, achieving 75.74% joint mode-intent accuracy and a 70.1% win rate against GPT-4o in human evaluation. These results demonstrate that pairing structurally diverse data with DPO enhances response quality and transition control, paving the way for more proactive and transition-aware conversational agents.",
    "citation": "Yoon et al. 2025. Beyond Task-Oriented and Chitchat Dialogues: Proactive and Transition-Aware Conversational Agents.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://still-with-you.github.io/tact-project-page/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "task-oriented dialogue"
      },
      {
        "@type": "Thing",
        "name": "open-domain chitchat"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "HYU-NLP Lab",
    "url": "https://github.com/HYU-NLP",
    "logo": "https://still-with-you.github.io/pages/tact/static/images/favicon.ico",
    "sameAs": [
      "https://github.com/HYU-NLP"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">
              <span class="title-line title-line-primary"><img class="title-icon" src="static/images/tact-icon-square.png" alt="TACT icon" loading="lazy">Beyond Task-Oriented and Chitchat Dialogues:</span>
              <span class="title-line">Proactive and Transition-Aware Conversational Agents</span>
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://still-with-you.github.io/" target="_blank">Yejin Yoon</a><sup>1</sup>,</span>
              <span class="author-block">Yuri Son<sup>1</sup>,</span>
              <span class="author-block">Namyoung So<sup>1</sup>,</span>
              <span class="author-block">Minseo Kim<sup>1</sup>,</span>
              <br>
              <span class="author-block">Minsoo Cho<sup>2</sup>,</span>
              <span class="author-block">Chanhee Park<sup>2</sup>,</span>
              <span class="author-block">Seungshin Lee<sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://galsang.github.io/" target="_blank">Taeuk Kim</a><sup>1*</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup><a href="https://sites.google.com/view/hyu-nlp/" target="_blank">Hanyang University</a>&nbsp;&nbsp;&nbsp;<sup>2</sup><a href="https://www.hyundai.com/worldwide/en" target="_blank">Hyundai Motor Company</a></span>
              <span class="author-block conference-line"><br>EMNLP 2025 Main Conference</span>
              <span class="author-block corresponding-line"><br><sup>*</sup>Corresponding author</span>
            </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <div class="link-row">
                      <span class="link-block">
                        <a href="https://aclanthology.org/2025.emnlp-main.672/" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>ACL Anthology</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="https://github.com/HYU-NLP/TACT" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://huggingface.co/datasets/HYU-NLP/TACT" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>

                <span class="link-block">
                  <a href="https://arxiv.org/abs/2511.08835" target="_blank"
                  class="external-link button is-normal is-rounded is-muted">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                    <span class="link-block">
                      <a href="static/pdfs/tact-slides.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-muted">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Slides</span>
                    </a>
                  </span>

            <span class="link-block">
              <a href="static/pdfs/tact-poster.pdf" target="_blank"
              class="external-link button is-normal is-rounded is-muted">
              <span class="icon">
                <i class="fas fa-image"></i>
              </span>
              <span>Poster</span>
            </a>
          </span>
            </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure class="image">
        <img src="static/images/tact_overview.png" alt="TACT overview figure" loading="lazy">
      </figure>
      <h2 class="subtitle has-text-centered">
        Only the TACT-trained agent successfully resumes the original task after a user-initiated chitchat, demonstrating both transition awareness and proactive flow control.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Conversational agents have traditionally been developed for either task-oriented dialogue (TOD) or open-ended chitchat, with limited progress in unifying the two. Yet, real-world conversations naturally involve fluid transitions between these modes. To address this gap, we introduce TACT (TOD-And-Chitchat Transition), a dataset designed for transition-aware dialogue modeling that incorporates structurally diverse and integrated mode flows. TACT supports both user- and agent-driven mode switches, enabling robust modeling of complex conversational dynamics. To evaluate an agent's ability to initiate and recover from mode transitions, we propose two new metrics: Switch and Recovery. Models trained on TACT outperform baselines in both intent detection and mode transition handling. Moreover, applying Direct Preference Optimization (DPO) to TACT-trained models yields additional gains, achieving 75.74% joint mode-intent accuracy and a 70.1% win rate against GPT-4o in human evaluation. These results demonstrate that pairing structurally diverse data with DPO enhances response quality and transition control, paving the way for more proactive and transition-aware conversational agents.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Narrative -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths content overview-content">
        <h2 class="title is-3 overview-title">Overview</h2>

        <h3 class="title is-4">Motivation</h3>
        <div class="narrative-block pref-align">
          <div class="narrative-text">
            <p>
              Most conversational agents are designed around a single interaction mode, either task-oriented dialogue or open-domain chitchat. In practice, however, real conversations move fluidly between these modes. Users often digress from a task to share personal thoughts or casual remarks, then expect the agent to naturally resume the original goal. Existing datasets and models largely fail to capture this behavior, as they either disallow mode transitions or treat them as one-off interruptions without recovery.
            </p>
          </div>
          <div class="narrative-figures">
            <figure class="figure-card section-figure figure-shift-right">
              <img src="static/images/tact-motivation.png" alt="Dialogue flow comparison across TACT" loading="lazy">
              <figcaption>Figure 1. Comparison of dialogue flow structures across datasets.</figcaption>
            </figure>
          </div>
        </div>

        <h3 class="title is-4">Research Focus</h3>
        <p>
          This paper studies conversational agents that can explicitly handle dialogue mode transitions as part of a continuous interaction. We frame transition handling not as a binary classification problem, but as a dialogue-level capability that requires both awareness and initiative. A transition-aware agent should detect when the dialogue mode changes, respond appropriately within the new mode, and proactively guide the conversation back to the task when the context allows.
        </p>

        <h3 class="title is-4">Dataset: TACT</h3>
        <figure class="figure-card section-figure full-width figure-shift">
          <img src="static/images/tact-construction-flow.png" alt="TACT construction flow" loading="lazy">
          <figcaption class="caption-center">Figure 3. TACT construction steps for TCT and CTC flows.</figcaption>
        </figure>
        <div class="narrative-block">
          <div class="narrative-text">
            <p>
              To support this research, we introduce TACT (TOD-And-Chitchat Transition), a dataset designed around recoverable, multi-turn mode transitions. Unlike prior resources that allow at most a single switch, TACT contains dialogues with multiple interwoven transitions between task-oriented dialogue and chitchat. These dialogues are constructed from MultiWOZ and SLURP using structured flow patterns such as TOD–Chitchat–TOD and Chitchat–TOD–Chitchat, making recovery an explicit and learnable phenomenon rather than an edge case.
            </p>
          </div>
          <div class="narrative-figures">
            <figure class="figure-card section-figure figure-shift">
              <img src="static/images/tact-data-dist.png" alt="Dialogue flow distribution across datasets" loading="lazy">
              <figcaption>Figure 4. Distribution of dialogue flow types in TACT.</figcaption>
            </figure>
          </div>
        </div>

        <h3 class="title is-4">Modeling and Evaluation</h3>
        <figure class="figure-card section-figure full-width">
          <img src="static/images/tact-metrics.png" alt="Switch and Recovery metrics" loading="lazy">
          <figcaption>Figure 6. Illustration of Switch and Recovery metrics.</figcaption>
        </figure>
        <p>
          We train unified dialogue models that jointly perform mode prediction, intent detection, and response generation. To directly evaluate transition behavior, we introduce two dialogue-level metrics: Switch, which measures whether an agent attempts a mode transition, and Recovery, which captures whether it successfully returns to a previously suspended mode. These metrics allow us to assess conversational flow control beyond standard task accuracy.
        </p>
        <figure class="figure-card section-figure full-width figure-shift">
          <img src="static/images/tact-table.png" alt="Comparison of modeling strategies" loading="lazy">
          <figcaption>Table 4. Comparison of modeling strategies. Preference-optimized models achieve the best balance between task accuracy, transition handling, and conversational quality.</figcaption>
        </figure>

        <h3 class="title is-4">Preference Alignment</h3>
        <figure class="figure-card section-figure full-width figure-shift">
          <img src="static/images/tact-laaj.png" alt="LLM-based evaluation results" loading="lazy">
          <figcaption>Figure 7. Preference-based evaluation results (LLM-as-a-judge).</figcaption>
        </figure>
        <div class="narrative-block">
          <div class="narrative-text">
            <br>
            <p>
              Beyond supervised fine-tuning, we apply Direct Preference Optimization (DPO) to align model behavior with human preferences, particularly for conversational qualities such as naturalness, engagement, and smooth transitions. Preference data is constructed by comparing model outputs under identical dialogue contexts and selecting responses preferred by human or LLM-based judges.
            </p>
          </div>
          <div class="narrative-figures">
            <figure class="figure-card section-figure figure-large figure-shift-right figure-tight">
              <img src="static/images/tact-human-eval.png" alt="Human evaluation results and win rates" loading="lazy">
              <figcaption>Figure 8. Preference-based evaluation results (human judgement).</figcaption>
            </figure>
          </div>
        </div>

        <h3 class="title is-4">Key Findings</h3>
        <p>
          Models trained on TACT are the only ones that exhibit meaningful switch and recovery behaviors across diverse dialogue flows. Preference optimization further improves chitchat quality and transition naturalness without sacrificing task performance. These results suggest that transition-aware data and preference-based learning are essential for building conversational agents that can manage dialogue flow in realistic, mixed-mode interactions.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End narrative -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@inproceedings{yoon-etal-2025-beyond,
  title = "Beyond Task-Oriented and Chitchat Dialogues: Proactive and Transition-Aware Conversational Agents",
  author = "Yoon, Yejin and Son, Yuri and So, Namyoung and Kim, Minseo and Cho, Minsoo and Park, Chanhee and Lee, Seungshin and Kim, Taeuk",
  editor = "Christodoulopoulos, Christos and Chakraborty, Tanmoy and Rose, Carolyn and Peng, Violet",
  booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
  month = nov,
  year = "2025",
  address = "Suzhou, China",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2025.emnlp-main.672/",
  doi = "10.18653/v1/2025.emnlp-main.672",
  pages = "13291--13317",
  ISBN = "979-8-89176-332-6"
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            Built with the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> (adapted from <a href="https://nerfies.github.io" target="_blank">Nerfies</a>).
            Licensed under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
